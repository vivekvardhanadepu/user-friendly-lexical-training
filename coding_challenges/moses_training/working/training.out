nohup: ignoring input
Using SCRIPTS_ROOTDIR: /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts
Using single-thread GIZA
using pigz 
(1) preparing corpus @ Wed Mar 24 17:28:18 IST 2021
Executing: mkdir -p /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus
(1.0) selecting factors @ Wed Mar 24 17:28:18 IST 2021
(1.1) running mkcls  @ Wed Mar 24 17:28:18 IST 2021
/home/vivek/Documents/FOSS/apertium/GIZA++/bin/mkcls -c50 -n2 -p/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/corpus/news-commentary-v8.fr-en.clean.fr -V/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr.vcb.classes opt
Executing: /home/vivek/Documents/FOSS/apertium/GIZA++/bin/mkcls -c50 -n2 -p/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/corpus/news-commentary-v8.fr-en.clean.fr -V/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 831

start-costs: MEAN: 18340.4 (18200.6-18480.1)  SIGMA:139.773   
  end-costs: MEAN: 14863.1 (14855.5-14870.6)  SIGMA:7.55013   
   start-pp: MEAN: 122.107 (115.554-128.66)  SIGMA:6.55299   
     end-pp: MEAN: 32.0428 (31.9498-32.1357)  SIGMA:0.0929771   
 iterations: MEAN: 20096 (19693-20499)  SIGMA:403   
       time: MEAN: 0.181689 (0.179871-0.183507)  SIGMA:0.001818   
(1.1) running mkcls  @ Wed Mar 24 17:28:18 IST 2021
/home/vivek/Documents/FOSS/apertium/GIZA++/bin/mkcls -c50 -n2 -p/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/corpus/news-commentary-v8.fr-en.clean.en -V/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en.vcb.classes opt
Executing: /home/vivek/Documents/FOSS/apertium/GIZA++/bin/mkcls -c50 -n2 -p/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/corpus/news-commentary-v8.fr-en.clean.en -V/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 792

start-costs: MEAN: 14925.5 (14919.6-14931.3)  SIGMA:5.8293   
  end-costs: MEAN: 12098.9 (12090-12107.8)  SIGMA:8.91967   
   start-pp: MEAN: 131.347 (130.995-131.7)  SIGMA:0.352677   
     end-pp: MEAN: 35.7261 (35.5793-35.8728)  SIGMA:0.146782   
 iterations: MEAN: 19162 (18937-19387)  SIGMA:225   
       time: MEAN: 0.187034 (0.172179-0.201889)  SIGMA:0.014855   
(1.2) creating vcb file /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr.vcb @ Wed Mar 24 17:28:19 IST 2021
(1.2) creating vcb file /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en.vcb @ Wed Mar 24 17:28:19 IST 2021
(1.3) numberizing corpus /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr-en-int-train.snt @ Wed Mar 24 17:28:19 IST 2021
(1.3) numberizing corpus /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en-fr-int-train.snt @ Wed Mar 24 17:28:19 IST 2021
(2) running giza @ Wed Mar 24 17:28:19 IST 2021
(2.1a) running snt2cooc fr-en @ Wed Mar 24 17:28:19 IST 2021

Executing: mkdir -p /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.fr-en
Executing: /home/vivek/Documents/FOSS/apertium/GIZA++/bin/snt2cooc.out /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en.vcb /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr.vcb /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr-en-int-train.snt > /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.fr-en/fr-en.cooc
/home/vivek/Documents/FOSS/apertium/GIZA++/bin/snt2cooc.out /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en.vcb /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr.vcb /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr-en-int-train.snt > /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.fr-en/fr-en.cooc
END.
(2.1b) running giza fr-en @ Wed Mar 24 17:28:19 IST 2021
/home/vivek/Documents/FOSS/apertium/GIZA++/bin/GIZA++  -CoocurrenceFile /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.fr-en/fr-en.cooc -c /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.fr-en/fr-en -onlyaldumps 1 -p0 0.999 -s /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en.vcb -t /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr.vcb
Executing: /home/vivek/Documents/FOSS/apertium/GIZA++/bin/GIZA++  -CoocurrenceFile /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.fr-en/fr-en.cooc -c /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.fr-en/fr-en -onlyaldumps 1 -p0 0.999 -s /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en.vcb -t /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr.vcb
/home/vivek/Documents/FOSS/apertium/GIZA++/bin/GIZA++  -CoocurrenceFile /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.fr-en/fr-en.cooc -c /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr-en-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.fr-en/fr-en -onlyaldumps 1 -p0 0.999 -s /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en.vcb -t /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr.vcb
Parameter 'coocurrencefile' changed from '' to '/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.fr-en/fr-en.cooc'
Parameter 'c' changed from '' to '/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr-en-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '2021-03-24.172819.vivek' to '/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.fr-en/fr-en'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en.vcb'
Parameter 't' changed from '' to '/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2021-03-24.172819.vivek.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.fr-en/fr-en  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr-en-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en.vcb  (source vocabulary file name)
t = /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2021-03-24.172819.vivek.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.fr-en/fr-en  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr-en-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en.vcb  (source vocabulary file name)
t = /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en.vcb
Reading vocabulary file from:/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr.vcb
Source vocabulary list has 793 unique tokens 
Target vocabulary list has 832 unique tokens 
Calculating vocabulary frequencies from corpus /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr-en-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 100 sentence pairs.
 Train total # sentence pairs (weighted): 100
Size of source portion of the training corpus: 2071 tokens
Size of the target portion of the training corpus: 2502 tokens 
In source portion of the training corpus, only 792 unique tokens appeared
In target portion of the training corpus, only 830 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 2502/(2171-100)== 1.20811
There are 36829 36829 entries in table
==========================================================
Model1 Training Started at: Wed Mar 24 17:28:19 2021

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 9.87508 PERPLEXITY 939.062
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 14.4097 PERPLEXITY 21764.7
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 5.72516 PERPLEXITY 52.8986
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 9.11954 PERPLEXITY 556.232
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 5.50302 PERPLEXITY 45.3496
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 8.57583 PERPLEXITY 381.577
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 5.37202 PERPLEXITY 41.4133
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 8.10166 PERPLEXITY 274.691
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 5.28991 PERPLEXITY 39.1219
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 7.7694 PERPLEXITY 218.184
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 0 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 792  #classes: 51
Read classes: #words: 831  #classes: 51

==========================================================
Hmm Training Started at: Wed Mar 24 17:28:19 2021

-----------
Hmm: Iteration 1
A/D table contains 27105 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 5.21926 PERPLEXITY 37.2523
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 7.53516 PERPLEXITY 185.486

Hmm Iteration: 1 took: 0 seconds

-----------
Hmm: Iteration 2
A/D table contains 27105 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 5.23835 PERPLEXITY 37.7485
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 6.79757 PERPLEXITY 111.243

Hmm Iteration: 2 took: 0 seconds

-----------
Hmm: Iteration 3
A/D table contains 27105 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 4.93924 PERPLEXITY 30.6802
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 6.02666 PERPLEXITY 65.1935

Hmm Iteration: 3 took: 0 seconds

-----------
Hmm: Iteration 4
A/D table contains 27105 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 4.61858 PERPLEXITY 24.5658
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 5.40214 PERPLEXITY 42.2869

Hmm Iteration: 4 took: 0 seconds

-----------
Hmm: Iteration 5
A/D table contains 27105 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 4.29439 PERPLEXITY 19.6219
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 4.85225 PERPLEXITY 28.885

Hmm Iteration: 5 took: 0 seconds

Entire Hmm Training took: 0 seconds
==========================================================
Read classes: #words: 792  #classes: 51
Read classes: #words: 831  #classes: 51
Read classes: #words: 792  #classes: 51
Read classes: #words: 831  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Wed Mar 24 17:28:19 2021


---------------------
THTo3: Iteration 1
#centers(pre/hillclimbed/real): 1 1 1  #al: 928.92 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 27105 parameters.
A/D table contains 29816 parameters.
NTable contains 7930 parameter.
p0_count is 2000.82 and p1 is 250.588; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 4.13068 PERPLEXITY 17.517
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 4.24769 PERPLEXITY 18.9968

THTo3 Viterbi Iteration : 1 took: 0 seconds

---------------------
Model3: Iteration 2
#centers(pre/hillclimbed/real): 1 1 1  #al: 931.8 #alsophisticatedcountcollection: 0 #hcsteps: 2.97
#peggingImprovements: 0
A/D table contains 27105 parameters.
A/D table contains 29804 parameters.
NTable contains 7930 parameter.
p0_count is 2250.82 and p1 is 125.591; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 4.02489 PERPLEXITY 16.2785
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 4.08599 PERPLEXITY 16.9827

Model3 Viterbi Iteration : 2 took: 0 seconds

---------------------
Model3: Iteration 3
#centers(pre/hillclimbed/real): 1 1 1  #al: 932.53 #alsophisticatedcountcollection: 0 #hcsteps: 3.42
#peggingImprovements: 0
A/D table contains 27105 parameters.
A/D table contains 29675 parameters.
NTable contains 7930 parameter.
p0_count is 2354.8 and p1 is 73.6015; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.68645 PERPLEXITY 12.8745
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 3.73804 PERPLEXITY 13.3433

Model3 Viterbi Iteration : 3 took: 0 seconds

---------------------
T3To4: Iteration 4
#centers(pre/hillclimbed/real): 1 1 1  #al: 933.01 #alsophisticatedcountcollection: 23.87 #hcsteps: 3.49
#peggingImprovements: 0
D4 table contains 326018 parameters.
A/D table contains 27105 parameters.
A/D table contains 29525 parameters.
NTable contains 7930 parameter.
p0_count is 2407.27 and p1 is 47.3656; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 3.53636 PERPLEXITY 11.6025
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 3.5801 PERPLEXITY 11.9597

T3To4 Viterbi Iteration : 4 took: 0 seconds

---------------------
Model4: Iteration 5
#centers(pre/hillclimbed/real): 1 1 1  #al: 933.43 #alsophisticatedcountcollection: 17.43 #hcsteps: 3.13
#peggingImprovements: 0
D4 table contains 326018 parameters.
A/D table contains 27105 parameters.
A/D table contains 28880 parameters.
NTable contains 7930 parameter.
p0_count is 2401.29 and p1 is 50.3534; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.88775 PERPLEXITY 14.8023
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 3.91591 PERPLEXITY 15.0941

Model4 Viterbi Iteration : 5 took: 0 seconds

---------------------
Model4: Iteration 6
#centers(pre/hillclimbed/real): 1 1 1  #al: 933.66 #alsophisticatedcountcollection: 12.64 #hcsteps: 3.11
#peggingImprovements: 0
D4 table contains 326018 parameters.
A/D table contains 27105 parameters.
A/D table contains 28502 parameters.
NTable contains 7930 parameter.
p0_count is 2420.27 and p1 is 40.866; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.73587 PERPLEXITY 13.3232
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 3.75723 PERPLEXITY 13.5219

Model4 Viterbi Iteration : 6 took: 0 seconds
H333444 Training Finished at: Wed Mar 24 17:28:19 2021


Entire Viterbi H333444 Training took: 0 seconds
==========================================================

Entire Training took: 0 seconds
Program Finished at: Wed Mar 24 17:28:19 2021

==========================================================
Executing: rm -f /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.fr-en/fr-en.A3.final.gz
Executing: pigz /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.fr-en/fr-en.A3.final
(2.1a) running snt2cooc en-fr @ Wed Mar 24 17:28:19 IST 2021

Executing: mkdir -p /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.en-fr
Executing: /home/vivek/Documents/FOSS/apertium/GIZA++/bin/snt2cooc.out /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr.vcb /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en.vcb /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en-fr-int-train.snt > /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.en-fr/en-fr.cooc
/home/vivek/Documents/FOSS/apertium/GIZA++/bin/snt2cooc.out /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr.vcb /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en.vcb /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en-fr-int-train.snt > /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.en-fr/en-fr.cooc
END.
(2.1b) running giza en-fr @ Wed Mar 24 17:28:19 IST 2021
/home/vivek/Documents/FOSS/apertium/GIZA++/bin/GIZA++  -CoocurrenceFile /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.en-fr/en-fr.cooc -c /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en-fr-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.en-fr/en-fr -onlyaldumps 1 -p0 0.999 -s /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr.vcb -t /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en.vcb
Executing: /home/vivek/Documents/FOSS/apertium/GIZA++/bin/GIZA++  -CoocurrenceFile /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.en-fr/en-fr.cooc -c /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en-fr-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.en-fr/en-fr -onlyaldumps 1 -p0 0.999 -s /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr.vcb -t /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en.vcb
/home/vivek/Documents/FOSS/apertium/GIZA++/bin/GIZA++  -CoocurrenceFile /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.en-fr/en-fr.cooc -c /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en-fr-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.en-fr/en-fr -onlyaldumps 1 -p0 0.999 -s /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr.vcb -t /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en.vcb
Parameter 'coocurrencefile' changed from '' to '/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.en-fr/en-fr.cooc'
Parameter 'c' changed from '' to '/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en-fr-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '2021-03-24.172819.vivek' to '/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.en-fr/en-fr'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr.vcb'
Parameter 't' changed from '' to '/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2021-03-24.172819.vivek.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.en-fr/en-fr  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en-fr-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr.vcb  (source vocabulary file name)
t = /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2021-03-24.172819.vivek.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.en-fr/en-fr  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en-fr-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr.vcb  (source vocabulary file name)
t = /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/fr.vcb
Reading vocabulary file from:/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en.vcb
Source vocabulary list has 832 unique tokens 
Target vocabulary list has 793 unique tokens 
Calculating vocabulary frequencies from corpus /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/corpus/en-fr-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 100 sentence pairs.
 Train total # sentence pairs (weighted): 100
Size of source portion of the training corpus: 2502 tokens
Size of the target portion of the training corpus: 2071 tokens 
In source portion of the training corpus, only 831 unique tokens appeared
In target portion of the training corpus, only 791 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 2071/(2602-100)== 0.827738
There are 36790 36790 entries in table
==========================================================
Model1 Training Started at: Wed Mar 24 17:28:19 2021

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 9.88163 PERPLEXITY 943.335
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 14.6903 PERPLEXITY 26437.3
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 5.7861 PERPLEXITY 55.1809
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 9.27316 PERPLEXITY 618.729
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 5.53317 PERPLEXITY 46.3074
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 8.68126 PERPLEXITY 410.507
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 5.3844 PERPLEXITY 41.77
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 8.17938 PERPLEXITY 289.895
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 5.2933 PERPLEXITY 39.2141
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 7.83429 PERPLEXITY 228.222
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 0 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 831  #classes: 51
Read classes: #words: 792  #classes: 51

==========================================================
Hmm Training Started at: Wed Mar 24 17:28:19 2021

-----------
Hmm: Iteration 1
A/D table contains 29740 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 5.23405 PERPLEXITY 37.6363
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 7.59548 PERPLEXITY 193.404

Hmm Iteration: 1 took: 0 seconds

-----------
Hmm: Iteration 2
A/D table contains 29740 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 5.2313 PERPLEXITY 37.5644
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 6.76927 PERPLEXITY 109.082

Hmm Iteration: 2 took: 1 seconds

-----------
Hmm: Iteration 3
A/D table contains 29740 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 4.879 PERPLEXITY 29.4257
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 5.91238 PERPLEXITY 60.2285

Hmm Iteration: 3 took: 0 seconds

-----------
Hmm: Iteration 4
A/D table contains 29740 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 4.45951 PERPLEXITY 22.0012
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 5.15885 PERPLEXITY 35.7247

Hmm Iteration: 4 took: 0 seconds

-----------
Hmm: Iteration 5
A/D table contains 29740 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 4.02859 PERPLEXITY 16.3202
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 4.5028 PERPLEXITY 22.6714

Hmm Iteration: 5 took: 0 seconds

Entire Hmm Training took: 1 seconds
==========================================================
Read classes: #words: 831  #classes: 51
Read classes: #words: 792  #classes: 51
Read classes: #words: 831  #classes: 51
Read classes: #words: 792  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Wed Mar 24 17:28:20 2021


---------------------
THTo3: Iteration 1
#centers(pre/hillclimbed/real): 1 1 1  #al: 831.12 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 29740 parameters.
A/D table contains 26764 parameters.
NTable contains 8320 parameter.
p0_count is 1748.86 and p1 is 161.07; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 3.65464 PERPLEXITY 12.5938
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 3.76999 PERPLEXITY 13.6421

THTo3 Viterbi Iteration : 1 took: 0 seconds

---------------------
Model3: Iteration 2
#centers(pre/hillclimbed/real): 1 1 1  #al: 832.86 #alsophisticatedcountcollection: 0 #hcsteps: 2.96
#peggingImprovements: 0
A/D table contains 29740 parameters.
A/D table contains 26742 parameters.
NTable contains 8320 parameter.
p0_count is 1953.96 and p1 is 58.5194; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.55686 PERPLEXITY 11.7685
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 3.62944 PERPLEXITY 12.3757

Model3 Viterbi Iteration : 2 took: 0 seconds

---------------------
Model3: Iteration 3
#centers(pre/hillclimbed/real): 1 1 1  #al: 833.73 #alsophisticatedcountcollection: 0 #hcsteps: 3.25
#peggingImprovements: 0
A/D table contains 29740 parameters.
A/D table contains 26688 parameters.
NTable contains 8320 parameter.
p0_count is 2003.65 and p1 is 33.6748; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.25827 PERPLEXITY 9.56835
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 3.31727 PERPLEXITY 9.96779

Model3 Viterbi Iteration : 3 took: 0 seconds

---------------------
T3To4: Iteration 4
#centers(pre/hillclimbed/real): 1 1 1  #al: 834.13 #alsophisticatedcountcollection: 20.2 #hcsteps: 3.4
#peggingImprovements: 0
D4 table contains 323176 parameters.
A/D table contains 29740 parameters.
A/D table contains 26526 parameters.
NTable contains 8320 parameter.
p0_count is 2024.46 and p1 is 23.2678; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 3.11995 PERPLEXITY 8.69361
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 3.17046 PERPLEXITY 9.00336

T3To4 Viterbi Iteration : 4 took: 0 seconds

---------------------
Model4: Iteration 5
#centers(pre/hillclimbed/real): 1 1 1  #al: 834.37 #alsophisticatedcountcollection: 17.33 #hcsteps: 3.42
#peggingImprovements: 0
D4 table contains 323176 parameters.
A/D table contains 29740 parameters.
A/D table contains 26284 parameters.
NTable contains 8320 parameter.
p0_count is 2026.06 and p1 is 22.4692; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.46861 PERPLEXITY 11.0702
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 3.50735 PERPLEXITY 11.3715

Model4 Viterbi Iteration : 5 took: 0 seconds

---------------------
Model4: Iteration 6
#centers(pre/hillclimbed/real): 1 1 1  #al: 834.47 #alsophisticatedcountcollection: 14.37 #hcsteps: 3.27
#peggingImprovements: 0
D4 table contains 323176 parameters.
A/D table contains 29740 parameters.
A/D table contains 25999 parameters.
NTable contains 8320 parameter.
p0_count is 2028.79 and p1 is 21.1046; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.36759 PERPLEXITY 10.3216
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 3.4004 PERPLEXITY 10.559

Model4 Viterbi Iteration : 6 took: 0 seconds
H333444 Training Finished at: Wed Mar 24 17:28:20 2021


Entire Viterbi H333444 Training took: 0 seconds
==========================================================

Entire Training took: 1 seconds
Program Finished at: Wed Mar 24 17:28:20 2021

==========================================================
Executing: rm -f /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.en-fr/en-fr.A3.final.gz
Executing: pigz /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.en-fr/en-fr.A3.final
(3) generate word alignment @ Wed Mar 24 17:28:20 IST 2021
Combining forward and inverted alignment from files:
  /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.fr-en/fr-en.A3.final.{bz2,gz}
  /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.en-fr/en-fr.A3.final.{bz2,gz}
Executing: mkdir -p /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model
Executing: /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/training/giza2bal.pl -d "pigz -cd /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.en-fr/en-fr.A3.final.gz" -i "pigz -cd /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/giza.fr-en/fr-en.A3.final.gz" |/home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/../bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="yes" > /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/aligned.grow-diag-final-and
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (1)
skip=<0> counts=<100>
(4) generate lexical translation table 0-0 @ Wed Mar 24 17:28:20 IST 2021
(/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/corpus/news-commentary-v8.fr-en.clean.fr,/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/corpus/news-commentary-v8.fr-en.clean.en,/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/lex)
!
Saved: /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/lex.f2e and /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/lex.e2f
FILE: /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/corpus/news-commentary-v8.fr-en.clean.en
FILE: /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/corpus/news-commentary-v8.fr-en.clean.fr
FILE: /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/aligned.grow-diag-final-and
(5) extract phrases @ Wed Mar 24 17:28:20 IST 2021
/home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/generic/extract-parallel.perl 3 split "sort    " /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/../bin/extract /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/corpus/news-commentary-v8.fr-en.clean.en /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/corpus/news-commentary-v8.fr-en.clean.fr /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/aligned.grow-diag-final-and /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
Executing: /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/generic/extract-parallel.perl 3 split "sort    " /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/../bin/extract /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/corpus/news-commentary-v8.fr-en.clean.en /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/corpus/news-commentary-v8.fr-en.clean.fr /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/aligned.grow-diag-final-and /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
MAX 7 1 0
Started Wed Mar 24 17:28:20 2021
using pigz 
isBSDSplit=0 
Executing: mkdir -p /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28380; ls -l /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28380 
total=100 line-per-split=34 
split -d -l 34 -a 7 /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/corpus/news-commentary-v8.fr-en.clean.en /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28380/target.split -d -l 34 -a 7 /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/corpus/news-commentary-v8.fr-en.clean.fr /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28380/source.split -d -l 34 -a 7 /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/aligned.grow-diag-final-and /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28380/align.merging extract / extract.inv
gunzip -c /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28380/extract.0000000.o.gz /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28380/extract.0000001.o.gz /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28380/extract.0000002.o.gz  | LC_ALL=C sort     -T /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28380 2>> /dev/stderr | pigz -c > /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/extract.o.sorted.gz 2>> /dev/stderr 
gunzip -c /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28380/extract.0000000.inv.gz /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28380/extract.0000001.inv.gz /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28380/extract.0000002.inv.gz  | LC_ALL=C sort     -T /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28380 2>> /dev/stderr | pigz -c > /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/extract.inv.sorted.gz 2>> /dev/stderr 
gunzip -c /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28380/extract.0000000.gz /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28380/extract.0000001.gz /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28380/extract.0000002.gz  | LC_ALL=C sort     -T /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28380 2>> /dev/stderr | pigz -c > /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/extract.sorted.gz 2>> /dev/stderr 
Finished Wed Mar 24 17:28:21 2021
(6) score phrases @ Wed Mar 24 17:28:21 IST 2021
(6.1)  creating table half /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/phrase-table.half.f2e @ Wed Mar 24 17:28:21 IST 2021
/home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/generic/score-parallel.perl 3 "sort    " /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/../bin/score /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/extract.sorted.gz /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/lex.f2e /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/phrase-table.half.f2e.gz  0 
Executing: /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/generic/score-parallel.perl 3 "sort    " /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/../bin/score /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/extract.sorted.gz /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/lex.f2e /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/phrase-table.half.f2e.gz  0 
using pigz 
Started Wed Mar 24 17:28:21 2021
/home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/../bin/score /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28438/extract.0.gz /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/lex.f2e /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28438/phrase-table.half.0000000.gz  2>> /dev/stderr 
/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28438/run.0.sh/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28438/run.1.sh/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28438/run.2.shmv /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28438/phrase-table.half.0000000.gz /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/phrase-table.half.f2e.gzrm -rf /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28438 
Finished Wed Mar 24 17:28:21 2021
(6.3)  creating table half /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/phrase-table.half.e2f @ Wed Mar 24 17:28:21 IST 2021
/home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/generic/score-parallel.perl 3 "sort    " /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/../bin/score /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/extract.inv.sorted.gz /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/lex.e2f /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
Executing: /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/generic/score-parallel.perl 3 "sort    " /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/../bin/score /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/extract.inv.sorted.gz /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/lex.e2f /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
using pigz 
Started Wed Mar 24 17:28:21 2021
/home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/../bin/score /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28464/extract.0.gz /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/lex.e2f /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28464/phrase-table.half.0000000.gz --Inverse  2>> /dev/stderr 
/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28464/run.0.sh/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28464/run.2.sh/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28464/run.1.shgunzip -c /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28464/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28464  | pigz -c > /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/phrase-table.half.e2f.gz  2>> /dev/stderr rm -rf /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/tmp.28464 
Finished Wed Mar 24 17:28:21 2021
(6.6) consolidating the two halves @ Wed Mar 24 17:28:21 IST 2021
Executing: /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/../bin/consolidate /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/phrase-table.half.f2e.gz /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/phrase-table.half.e2f.gz /dev/stdout | pigz -c > /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/phrase-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables

Executing: rm -f /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/phrase-table.half.*
(7) learn reordering model @ Wed Mar 24 17:28:21 IST 2021
(7.1) [no factors] learn reordering model @ Wed Mar 24 17:28:21 IST 2021
(7.2) building tables @ Wed Mar 24 17:28:21 IST 2021
Executing: /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/../bin/lexical-reordering-score /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/extract.o.sorted.gz 0.5 /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/moses_training/working/train/model/reordering-table. --model "wbe msd wbe-msd-bidirectional-fe"
Lexical Reordering Scorer
scores lexical reordering models of several types (hierarchical, phrase-based and word-based-extraction
(8) learn generation model @ Wed Mar 24 17:28:21 IST 2021
  no generation model requested, skipping step
(9) create moses.ini @ Wed Mar 24 17:28:21 IST 2021
