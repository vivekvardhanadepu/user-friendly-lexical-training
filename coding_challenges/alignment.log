Using SCRIPTS_ROOTDIR: /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts
Using single-thread GIZA
using pigz 
(1) preparing corpus @ Sun Mar 28 02:34:37 IST 2021
Executing: mkdir -p ./corpus
(1.0) selecting factors @ Sun Mar 28 02:34:37 IST 2021
(1.1) running mkcls  @ Sun Mar 28 02:34:37 IST 2021
/home/vivek/Documents/FOSS/apertium/GIZA++/bin/mkcls -c50 -n2 -p/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/data-es-pt/Europarl.tag-clean.pt -V./corpus/pt.vcb.classes opt
Executing: /home/vivek/Documents/FOSS/apertium/GIZA++/bin/mkcls -c50 -n2 -p/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/data-es-pt/Europarl.tag-clean.pt -V./corpus/pt.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 3199

start-costs: MEAN: 147947 (147847-148046)  SIGMA:99.0754   
  end-costs: MEAN: 127420 (127307-127532)  SIGMA:112.757   
   start-pp: MEAN: 268.717 (267.048-270.386)  SIGMA:1.66883   
     end-pp: MEAN: 74.2139 (73.6894-74.7385)  SIGMA:0.524542   
 iterations: MEAN: 78118.5 (77660-78577)  SIGMA:458.5   
       time: MEAN: 0.799306 (0.790164-0.808449)  SIGMA:0.0091425   
(1.1) running mkcls  @ Sun Mar 28 02:34:39 IST 2021
/home/vivek/Documents/FOSS/apertium/GIZA++/bin/mkcls -c50 -n2 -p/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/data-es-pt/Europarl.tag-clean.es -V./corpus/es.vcb.classes opt
Executing: /home/vivek/Documents/FOSS/apertium/GIZA++/bin/mkcls -c50 -n2 -p/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/data-es-pt/Europarl.tag-clean.es -V./corpus/es.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 3154

start-costs: MEAN: 144973 (144931-145015)  SIGMA:42.2629   
  end-costs: MEAN: 124028 (123796-124261)  SIGMA:232.248   
   start-pp: MEAN: 277.24 (276.49-277.991)  SIGMA:0.750174   
     end-pp: MEAN: 72.5305 (71.452-73.6089)  SIGMA:1.07842   
 iterations: MEAN: 74927.5 (72185-77670)  SIGMA:2742.5   
       time: MEAN: 0.75078 (0.733225-0.768336)  SIGMA:0.0175555   
(1.2) creating vcb file ./corpus/pt.vcb @ Sun Mar 28 02:34:40 IST 2021
(1.2) creating vcb file ./corpus/es.vcb @ Sun Mar 28 02:34:40 IST 2021
(1.3) numberizing corpus ./corpus/pt-es-int-train.snt @ Sun Mar 28 02:34:40 IST 2021
(1.3) numberizing corpus ./corpus/es-pt-int-train.snt @ Sun Mar 28 02:34:40 IST 2021
(2) running giza @ Sun Mar 28 02:34:40 IST 2021
(2.1a) running snt2cooc pt-es @ Sun Mar 28 02:34:40 IST 2021

Executing: mkdir -p ./giza.pt-es
Executing: /home/vivek/Documents/FOSS/apertium/GIZA++/bin/snt2cooc.out ./corpus/es.vcb ./corpus/pt.vcb ./corpus/pt-es-int-train.snt > ./giza.pt-es/pt-es.cooc
/home/vivek/Documents/FOSS/apertium/GIZA++/bin/snt2cooc.out ./corpus/es.vcb ./corpus/pt.vcb ./corpus/pt-es-int-train.snt > ./giza.pt-es/pt-es.cooc
END.
(2.1b) running giza pt-es @ Sun Mar 28 02:34:40 IST 2021
/home/vivek/Documents/FOSS/apertium/GIZA++/bin/GIZA++  -CoocurrenceFile ./giza.pt-es/pt-es.cooc -c ./corpus/pt-es-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o ./giza.pt-es/pt-es -onlyaldumps 1 -p0 0.999 -s ./corpus/es.vcb -t ./corpus/pt.vcb
Executing: /home/vivek/Documents/FOSS/apertium/GIZA++/bin/GIZA++  -CoocurrenceFile ./giza.pt-es/pt-es.cooc -c ./corpus/pt-es-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o ./giza.pt-es/pt-es -onlyaldumps 1 -p0 0.999 -s ./corpus/es.vcb -t ./corpus/pt.vcb
/home/vivek/Documents/FOSS/apertium/GIZA++/bin/GIZA++  -CoocurrenceFile ./giza.pt-es/pt-es.cooc -c ./corpus/pt-es-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o ./giza.pt-es/pt-es -onlyaldumps 1 -p0 0.999 -s ./corpus/es.vcb -t ./corpus/pt.vcb
Parameter 'coocurrencefile' changed from '' to './giza.pt-es/pt-es.cooc'
Parameter 'c' changed from '' to './corpus/pt-es-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '2021-03-28.023440.vivek' to './giza.pt-es/pt-es'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to './corpus/es.vcb'
Parameter 't' changed from '' to './corpus/pt.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2021-03-28.023440.vivek.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = ./giza.pt-es/pt-es  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = ./corpus/pt-es-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = ./corpus/es.vcb  (source vocabulary file name)
t = ./corpus/pt.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2021-03-28.023440.vivek.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = ./giza.pt-es/pt-es  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = ./corpus/pt-es-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = ./corpus/es.vcb  (source vocabulary file name)
t = ./corpus/pt.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:./corpus/es.vcb
Reading vocabulary file from:./corpus/pt.vcb
Source vocabulary list has 3155 unique tokens 
Target vocabulary list has 3200 unique tokens 
Calculating vocabulary frequencies from corpus ./corpus/pt-es-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 699 sentence pairs.
 Train total # sentence pairs (weighted): 699
Size of source portion of the training corpus: 14920 tokens
Size of the target portion of the training corpus: 15254 tokens 
In source portion of the training corpus, only 3154 unique tokens appeared
In target portion of the training corpus, only 3198 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 15254/(15619-699)== 1.02239
There are 174184 174184 entries in table
==========================================================
Model1 Training Started at: Sun Mar 28 02:34:40 2021

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 11.8271 PERPLEXITY 3633.49
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 16.4324 PERPLEXITY 88439.1
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 5.98833 PERPLEXITY 63.4845
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 8.96233 PERPLEXITY 498.803
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 5.52009 PERPLEXITY 45.8893
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 8.0011 PERPLEXITY 256.195
Model 1 Iteration: 3 took: 1 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 5.29737 PERPLEXITY 39.3248
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 7.37439 PERPLEXITY 165.926
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 5.17833 PERPLEXITY 36.2103
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 6.99594 PERPLEXITY 127.641
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 1 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 3154  #classes: 51
Read classes: #words: 3199  #classes: 51

==========================================================
Hmm Training Started at: Sun Mar 28 02:34:41 2021

-----------
Hmm: Iteration 1
A/D table contains 28048 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 5.11043 PERPLEXITY 34.5456
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 6.75601 PERPLEXITY 108.084

Hmm Iteration: 1 took: 0 seconds

-----------
Hmm: Iteration 2
A/D table contains 28048 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 4.81794 PERPLEXITY 28.2061
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 5.61755 PERPLEXITY 49.0967

Hmm Iteration: 2 took: 0 seconds

-----------
Hmm: Iteration 3
A/D table contains 28048 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 4.01613 PERPLEXITY 16.1799
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 4.39254 PERPLEXITY 21.0032

Hmm Iteration: 3 took: 1 seconds

-----------
Hmm: Iteration 4
A/D table contains 28048 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 3.39919 PERPLEXITY 10.5502
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 3.60539 PERPLEXITY 12.1711

Hmm Iteration: 4 took: 0 seconds

-----------
Hmm: Iteration 5
A/D table contains 28048 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 3.11284 PERPLEXITY 8.65087
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 3.25354 PERPLEXITY 9.53701

Hmm Iteration: 5 took: 0 seconds

Entire Hmm Training took: 1 seconds
==========================================================
Read classes: #words: 3154  #classes: 51
Read classes: #words: 3199  #classes: 51
Read classes: #words: 3154  #classes: 51
Read classes: #words: 3199  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Sun Mar 28 02:34:42 2021


---------------------
THTo3: Iteration 1
#centers(pre/hillclimbed/real): 1 1 1  #al: 809.074 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 28048 parameters.
A/D table contains 27514 parameters.
NTable contains 31550 parameter.
p0_count is 12934.7 and p1 is 1159.64; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 2.85657 PERPLEXITY 7.24292
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 2.91658 PERPLEXITY 7.55052

THTo3 Viterbi Iteration : 1 took: 0 seconds

---------------------
Model3: Iteration 2
#centers(pre/hillclimbed/real): 1 1 1  #al: 810.492 #alsophisticatedcountcollection: 0 #hcsteps: 2.65665
#peggingImprovements: 0
A/D table contains 28048 parameters.
A/D table contains 27514 parameters.
NTable contains 31550 parameter.
p0_count is 14156.2 and p1 is 548.889; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 4.4559 PERPLEXITY 21.9462
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 4.51607 PERPLEXITY 22.8809

Model3 Viterbi Iteration : 2 took: 1 seconds

---------------------
Model3: Iteration 3
#centers(pre/hillclimbed/real): 1 1 1  #al: 810.557 #alsophisticatedcountcollection: 0 #hcsteps: 2.79971
#peggingImprovements: 0
A/D table contains 28048 parameters.
A/D table contains 27514 parameters.
NTable contains 31550 parameter.
p0_count is 14446.8 and p1 is 403.624; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 4.29003 PERPLEXITY 19.5626
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 4.33755 PERPLEXITY 20.2177

Model3 Viterbi Iteration : 3 took: 0 seconds

---------------------
T3To4: Iteration 4
#centers(pre/hillclimbed/real): 1 1 1  #al: 810.665 #alsophisticatedcountcollection: 26.3763 #hcsteps: 2.83834
#peggingImprovements: 0
D4 table contains 448224 parameters.
A/D table contains 28048 parameters.
A/D table contains 27514 parameters.
NTable contains 31550 parameter.
p0_count is 14563.9 and p1 is 345.074; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 4.2347 PERPLEXITY 18.8266
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 4.27788 PERPLEXITY 19.3986

T3To4 Viterbi Iteration : 4 took: 0 seconds

---------------------
Model4: Iteration 5
#centers(pre/hillclimbed/real): 1 1 1  #al: 810.661 #alsophisticatedcountcollection: 23.1187 #hcsteps: 2.33763
#peggingImprovements: 0
D4 table contains 448224 parameters.
A/D table contains 28048 parameters.
A/D table contains 27542 parameters.
NTable contains 31550 parameter.
p0_count is 14356.3 and p1 is 448.845; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.66757 PERPLEXITY 12.7072
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 3.697 PERPLEXITY 12.9691

Model4 Viterbi Iteration : 5 took: 1 seconds

---------------------
Model4: Iteration 6
#centers(pre/hillclimbed/real): 1 1 1  #al: 810.641 #alsophisticatedcountcollection: 17.8069 #hcsteps: 2.299
#peggingImprovements: 0
D4 table contains 448224 parameters.
A/D table contains 28048 parameters.
A/D table contains 27542 parameters.
NTable contains 31550 parameter.
p0_count is 14371.9 and p1 is 441.034; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.50671 PERPLEXITY 11.3665
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 3.52999 PERPLEXITY 11.5513

Model4 Viterbi Iteration : 6 took: 0 seconds
H333444 Training Finished at: Sun Mar 28 02:34:44 2021


Entire Viterbi H333444 Training took: 2 seconds
==========================================================

Entire Training took: 4 seconds
Program Finished at: Sun Mar 28 02:34:44 2021

==========================================================
Executing: rm -f ./giza.pt-es/pt-es.A3.final.gz
Executing: pigz ./giza.pt-es/pt-es.A3.final
(2.1a) running snt2cooc es-pt @ Sun Mar 28 02:34:44 IST 2021

Executing: mkdir -p ./giza.es-pt
Executing: /home/vivek/Documents/FOSS/apertium/GIZA++/bin/snt2cooc.out ./corpus/pt.vcb ./corpus/es.vcb ./corpus/es-pt-int-train.snt > ./giza.es-pt/es-pt.cooc
/home/vivek/Documents/FOSS/apertium/GIZA++/bin/snt2cooc.out ./corpus/pt.vcb ./corpus/es.vcb ./corpus/es-pt-int-train.snt > ./giza.es-pt/es-pt.cooc
END.
(2.1b) running giza es-pt @ Sun Mar 28 02:34:45 IST 2021
/home/vivek/Documents/FOSS/apertium/GIZA++/bin/GIZA++  -CoocurrenceFile ./giza.es-pt/es-pt.cooc -c ./corpus/es-pt-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o ./giza.es-pt/es-pt -onlyaldumps 1 -p0 0.999 -s ./corpus/pt.vcb -t ./corpus/es.vcb
Executing: /home/vivek/Documents/FOSS/apertium/GIZA++/bin/GIZA++  -CoocurrenceFile ./giza.es-pt/es-pt.cooc -c ./corpus/es-pt-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o ./giza.es-pt/es-pt -onlyaldumps 1 -p0 0.999 -s ./corpus/pt.vcb -t ./corpus/es.vcb
/home/vivek/Documents/FOSS/apertium/GIZA++/bin/GIZA++  -CoocurrenceFile ./giza.es-pt/es-pt.cooc -c ./corpus/es-pt-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o ./giza.es-pt/es-pt -onlyaldumps 1 -p0 0.999 -s ./corpus/pt.vcb -t ./corpus/es.vcb
Parameter 'coocurrencefile' changed from '' to './giza.es-pt/es-pt.cooc'
Parameter 'c' changed from '' to './corpus/es-pt-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '2021-03-28.023445.vivek' to './giza.es-pt/es-pt'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to './corpus/pt.vcb'
Parameter 't' changed from '' to './corpus/es.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2021-03-28.023445.vivek.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = ./giza.es-pt/es-pt  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = ./corpus/es-pt-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = ./corpus/pt.vcb  (source vocabulary file name)
t = ./corpus/es.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2021-03-28.023445.vivek.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = ./giza.es-pt/es-pt  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = ./corpus/es-pt-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = ./corpus/pt.vcb  (source vocabulary file name)
t = ./corpus/es.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:./corpus/pt.vcb
Reading vocabulary file from:./corpus/es.vcb
Source vocabulary list has 3200 unique tokens 
Target vocabulary list has 3155 unique tokens 
Calculating vocabulary frequencies from corpus ./corpus/es-pt-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 699 sentence pairs.
 Train total # sentence pairs (weighted): 699
Size of source portion of the training corpus: 15254 tokens
Size of the target portion of the training corpus: 14920 tokens 
In source portion of the training corpus, only 3199 unique tokens appeared
In target portion of the training corpus, only 3153 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 14920/(15953-699)== 0.978104
There are 174139 174139 entries in table
==========================================================
Model1 Training Started at: Sun Mar 28 02:34:45 2021

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 11.8152 PERPLEXITY 3603.46
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 16.4552 PERPLEXITY 89850.6
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 6.01013 PERPLEXITY 64.451
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 8.99613 PERPLEXITY 510.629
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 5.5517 PERPLEXITY 46.906
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 8.04867 PERPLEXITY 264.784
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 5.32879 PERPLEXITY 40.1907
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 7.40608 PERPLEXITY 169.61
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 5.2069 PERPLEXITY 36.9346
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 7.01272 PERPLEXITY 129.133
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 0 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 3199  #classes: 51
Read classes: #words: 3154  #classes: 51

==========================================================
Hmm Training Started at: Sun Mar 28 02:34:45 2021

-----------
Hmm: Iteration 1
A/D table contains 27863 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 5.13608 PERPLEXITY 35.1653
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 6.76122 PERPLEXITY 108.475

Hmm Iteration: 1 took: 0 seconds

-----------
Hmm: Iteration 2
A/D table contains 27863 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 4.82512 PERPLEXITY 28.3468
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 5.62767 PERPLEXITY 49.4422

Hmm Iteration: 2 took: 0 seconds

-----------
Hmm: Iteration 3
A/D table contains 27863 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 3.99252 PERPLEXITY 15.9172
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 4.3714 PERPLEXITY 20.6977

Hmm Iteration: 3 took: 1 seconds

-----------
Hmm: Iteration 4
A/D table contains 27863 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 3.37215 PERPLEXITY 10.3542
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 3.58186 PERPLEXITY 11.9742

Hmm Iteration: 4 took: 0 seconds

-----------
Hmm: Iteration 5
A/D table contains 27863 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 3.09624 PERPLEXITY 8.55185
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 3.24234 PERPLEXITY 9.4633

Hmm Iteration: 5 took: 0 seconds

Entire Hmm Training took: 1 seconds
==========================================================
Read classes: #words: 3199  #classes: 51
Read classes: #words: 3154  #classes: 51
Read classes: #words: 3199  #classes: 51
Read classes: #words: 3154  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Sun Mar 28 02:34:46 2021


---------------------
THTo3: Iteration 1
#centers(pre/hillclimbed/real): 1 1 1  #al: 800.684 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 27863 parameters.
A/D table contains 27658 parameters.
NTable contains 32000 parameter.
p0_count is 13103.9 and p1 is 908.049; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 2.83917 PERPLEXITY 7.15608
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 2.90162 PERPLEXITY 7.47263

THTo3 Viterbi Iteration : 1 took: 1 seconds

---------------------
Model3: Iteration 2
#centers(pre/hillclimbed/real): 1 1 1  #al: 801.658 #alsophisticatedcountcollection: 0 #hcsteps: 2.53219
#peggingImprovements: 0
A/D table contains 27863 parameters.
A/D table contains 27658 parameters.
NTable contains 32000 parameter.
p0_count is 14119.2 and p1 is 400.4; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 4.37194 PERPLEXITY 20.7054
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 4.43178 PERPLEXITY 21.5823

Model3 Viterbi Iteration : 2 took: 0 seconds

---------------------
Model3: Iteration 3
#centers(pre/hillclimbed/real): 1 1 1  #al: 801.765 #alsophisticatedcountcollection: 0 #hcsteps: 2.69528
#peggingImprovements: 0
A/D table contains 27863 parameters.
A/D table contains 27658 parameters.
NTable contains 32000 parameter.
p0_count is 14315.3 and p1 is 302.35; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 4.21189 PERPLEXITY 18.5313
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 4.25854 PERPLEXITY 19.1403

Model3 Viterbi Iteration : 3 took: 0 seconds

---------------------
T3To4: Iteration 4
#centers(pre/hillclimbed/real): 1 1 1  #al: 801.798 #alsophisticatedcountcollection: 24.1617 #hcsteps: 2.76681
#peggingImprovements: 0
D4 table contains 450660 parameters.
A/D table contains 27863 parameters.
A/D table contains 27658 parameters.
NTable contains 32000 parameter.
p0_count is 14409.2 and p1 is 255.387; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 4.14872 PERPLEXITY 17.7374
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 4.18939 PERPLEXITY 18.2445

T3To4 Viterbi Iteration : 4 took: 0 seconds

---------------------
Model4: Iteration 5
#centers(pre/hillclimbed/real): 1 1 1  #al: 801.803 #alsophisticatedcountcollection: 19.8712 #hcsteps: 2.2618
#peggingImprovements: 0
D4 table contains 450660 parameters.
A/D table contains 27863 parameters.
A/D table contains 27672 parameters.
NTable contains 32000 parameter.
p0_count is 14322 and p1 is 298.982; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.58317 PERPLEXITY 11.9851
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 3.60975 PERPLEXITY 12.2079

Model4 Viterbi Iteration : 5 took: 1 seconds

---------------------
Model4: Iteration 6
#centers(pre/hillclimbed/real): 1 1 1  #al: 801.823 #alsophisticatedcountcollection: 15.5565 #hcsteps: 2.16166
#peggingImprovements: 0
D4 table contains 450660 parameters.
A/D table contains 27863 parameters.
A/D table contains 27672 parameters.
NTable contains 32000 parameter.
p0_count is 14333.9 and p1 is 293.048; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.42412 PERPLEXITY 10.734
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 3.44468 PERPLEXITY 10.8881

Model4 Viterbi Iteration : 6 took: 0 seconds
H333444 Training Finished at: Sun Mar 28 02:34:48 2021


Entire Viterbi H333444 Training took: 2 seconds
==========================================================

Entire Training took: 3 seconds
Program Finished at: Sun Mar 28 02:34:48 2021

==========================================================
Executing: rm -f ./giza.es-pt/es-pt.A3.final.gz
Executing: pigz ./giza.es-pt/es-pt.A3.final
(3) generate word alignment @ Sun Mar 28 02:34:48 IST 2021
Combining forward and inverted alignment from files:
  ./giza.pt-es/pt-es.A3.final.{bz2,gz}
  ./giza.es-pt/es-pt.A3.final.{bz2,gz}
Executing: mkdir -p ./model
Executing: /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/training/giza2bal.pl -d "pigz -cd ./giza.es-pt/es-pt.A3.final.gz" -i "pigz -cd ./giza.pt-es/pt-es.A3.final.gz" |/home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/../bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="yes" > ./model/aligned.grow-diag-final-and
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (1)
skip=<0> counts=<699>
(4) generate lexical translation table 0-0 @ Sun Mar 28 02:34:49 IST 2021
(/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/data-es-pt/Europarl.tag-clean.pt,/home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/data-es-pt/Europarl.tag-clean.es,./model/lex)
!
Saved: ./model/lex.f2e and ./model/lex.e2f
FILE: /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/data-es-pt/Europarl.tag-clean.es
FILE: /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/data-es-pt/Europarl.tag-clean.pt
FILE: ./model/aligned.grow-diag-final-and
(5) extract phrases @ Sun Mar 28 02:34:49 IST 2021
/home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/../bin/extract /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/data-es-pt/Europarl.tag-clean.es /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/data-es-pt/Europarl.tag-clean.pt ./model/aligned.grow-diag-final-and ./model/extract 7 orientation --model wbe-msd --GZOutput 
Executing: /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/../bin/extract /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/data-es-pt/Europarl.tag-clean.es /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/data-es-pt/Europarl.tag-clean.pt ./model/aligned.grow-diag-final-and ./model/extract 7 orientation --model wbe-msd --GZOutput 
MAX 7 1 0
Started Sun Mar 28 02:34:49 2021
using pigz 
isBSDSplit=0 
Executing: mkdir -p ./model/tmp.50542; ls -l ./model/tmp.50542 
total=699 line-per-split=175 
split -d -l 175 -a 7 /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/data-es-pt/Europarl.tag-clean.es ./model/tmp.50542/target.split -d -l 175 -a 7 /home/vivek/Documents/FOSS/apertium/user-friendly-lexical-training/coding_challenges/data-es-pt/Europarl.tag-clean.pt ./model/tmp.50542/source.split -d -l 175 -a 7 ./model/aligned.grow-diag-final-and ./model/tmp.50542/align.PhraseExtract v1.5, written by Philipp Koehn et al.
phrase extraction from an aligned parallel corpus
PhraseExtract v1.5, written by Philipp Koehn et al.PhraseExtract v1.5, written by Philipp Koehn et al.
phrase extraction from an aligned parallel corpus

phrase extraction from an aligned parallel corpus
PhraseExtract v1.5, written by Philipp Koehn et al.
phrase extraction from an aligned parallel corpus




merging extract / extract.inv
gunzip -c ./model/tmp.50542/extract.0000000.gz ./model/tmp.50542/extract.0000001.gz ./model/tmp.50542/extract.0000002.gz ./model/tmp.50542/extract.0000003.gz  | LC_ALL=C sort     -T ./model/tmp.50542 2>> /dev/stderr | pigz -c > ./model/extract.sorted.gz 2>> /dev/stderr 
gunzip -c ./model/tmp.50542/extract.0000000.inv.gz ./model/tmp.50542/extract.0000001.inv.gz ./model/tmp.50542/extract.0000002.inv.gz ./model/tmp.50542/extract.0000003.inv.gz  | LC_ALL=C sort     -T ./model/tmp.50542 2>> /dev/stderr | pigz -c > ./model/extract.inv.sorted.gz 2>> /dev/stderr 
gunzip -c ./model/tmp.50542/extract.0000000.o.gz ./model/tmp.50542/extract.0000001.o.gz ./model/tmp.50542/extract.0000002.o.gz ./model/tmp.50542/extract.0000003.o.gz  | LC_ALL=C sort     -T ./model/tmp.50542 2>> /dev/stderr | pigz -c > ./model/extract.o.sorted.gz 2>> /dev/stderr 
Finished Sun Mar 28 02:34:50 2021
(6) score phrases @ Sun Mar 28 02:34:50 IST 2021
(6.1)  creating table half ./model/phrase-table.half.f2e @ Sun Mar 28 02:34:50 IST 2021
/home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/../bin/score ./model/extract.sorted.gz ./model/lex.f2e ./model/phrase-table.half.f2e.gz  0 
Executing: /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/../bin/score ./model/extract.sorted.gz ./model/lex.f2e ./model/phrase-table.half.f2e.gz  0 
using pigz 
Started Sun Mar 28 02:34:50 2021
/home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/../bin/score ./model/tmp.50606/extract.0.gz ./model/lex.f2e ./model/tmp.50606/phrase-table.half.0000000.gz  2>> /dev/stderr 
./model/tmp.50606/run.0.sh./model/tmp.50606/run.3.sh./model/tmp.50606/run.2.sh./model/tmp.50606/run.1.shScore v2.1 -- scoring methods for extracted rules
Loading lexical translation table from ./model/lex.f2e

mv ./model/tmp.50606/phrase-table.half.0000000.gz ./model/phrase-table.half.f2e.gzrm -rf ./model/tmp.50606 
Finished Sun Mar 28 02:34:51 2021
(6.3)  creating table half ./model/phrase-table.half.e2f @ Sun Mar 28 02:34:51 IST 2021
/home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/../bin/score ./model/extract.inv.sorted.gz ./model/lex.e2f ./model/phrase-table.half.e2f.gz --Inverse 1 
Executing: /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/../bin/score ./model/extract.inv.sorted.gz ./model/lex.e2f ./model/phrase-table.half.e2f.gz --Inverse 1 
using pigz 
Started Sun Mar 28 02:34:51 2021
/home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/../bin/score ./model/tmp.50637/extract.0.gz ./model/lex.e2f ./model/tmp.50637/phrase-table.half.0000000.gz --Inverse  2>> /dev/stderr 
./model/tmp.50637/run.0.sh./model/tmp.50637/run.1.sh./model/tmp.50637/run.3.sh./model/tmp.50637/run.2.shScore v2.1 -- scoring methods for extracted rules
using inverse mode
Loading lexical translation table from ./model/lex.e2f

gunzip -c ./model/tmp.50637/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T ./model/tmp.50637  | pigz -c > ./model/phrase-table.half.e2f.gz  2>> /dev/stderr rm -rf ./model/tmp.50637 
Finished Sun Mar 28 02:34:52 2021
(6.6) consolidating the two halves @ Sun Mar 28 02:34:52 IST 2021
Executing: /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/../bin/consolidate ./model/phrase-table.half.f2e.gz ./model/phrase-table.half.e2f.gz /dev/stdout | pigz -c > ./model/phrase-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables

Executing: rm -f ./model/phrase-table.half.*
(7) learn reordering model @ Sun Mar 28 02:34:52 IST 2021
(7.1) [no factors] learn reordering model @ Sun Mar 28 02:34:52 IST 2021
(7.2) building tables @ Sun Mar 28 02:34:52 IST 2021
Executing: /home/vivek/Documents/FOSS/apertium/mosesdecoder/scripts/../bin/lexical-reordering-score ./model/extract.o.sorted.gz 0.5 ./model/reordering-table. --model "wbe msd wbe-msd-bidirectional-fe"
Lexical Reordering Scorer
scores lexical reordering models of several types (hierarchical, phrase-based and word-based-extraction
(8) learn generation model @ Sun Mar 28 02:34:53 IST 2021
  no generation model requested, skipping step
(9) create moses.ini @ Sun Mar 28 02:34:53 IST 2021
